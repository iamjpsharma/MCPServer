# Fremem (formerly MCP Memory Server)

![License](https://img.shields.io/github/license/iamjpsharma/fremem?color=blue)
![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![Release](https://img.shields.io/github/v/release/iamjpsharma/fremem?include_prereleases)

A persistent vector memory server for Windsurf, VS Code, and other MCP-compliant editors.

## üåü Philosophy

- **Privacy-first, local-first AI memory:** Your data stays on your machine.
- **No vendor lock-in:** Uses open standards and local files.
- **Built for MCP:** Designed specifically to enhance Windsurf, Cursor, and other MCP-compatible IDEs.

## ‚ÑπÔ∏è Status (v0.2.0)

**Stable:**
- ‚úÖ Local MCP memory with Windsurf/Cursor
- ‚úÖ Multi-project isolation
- ‚úÖ Ingestion of Markdown docs

**Not stable yet:**
- üöß Auto-ingest (file watching)
- üöß Memory pruning
- üöß Remote sync

> **Note:** There are two ways to run this server:
> 1. **Local IDE (stdio)**: Used by Windsurf/Cursor (default).
> 2. **Docker/Server (HTTP)**: Used for remote deployments or Docker (exposes port 8000).


## üè• Health Check

To verify the server binary runs correctly:

```bash
# From within the virtual environment
python -m fremem.server --help
```


## ‚úÖ Quickstart (5-Minute Setup)

There are two ways to set this up: **Global Install** (recommended for ease of use) or **Local Dev**.

### Option A: Global Install (Like `npm -g`)

This method allows you to run `fremem` from anywhere without managing virtual environments manually.

**Prerequisites:**
- Python 3.10+
- [`pipx`](https://github.com/pypa/pipx) (Recommended: `brew install pipx` on macOS)

```bash
# Install directly from GitHub
pipx install git+https://github.com/iamjpsharma/fremem.git

# Verify installation
fremem --help
```

**Configure Windsurf / VS Code:**

Since `pipx` puts the executable in your PATH, the config is simpler:

```json
{
  "mcpServers": {
    "memory": {
      "command": "fremem",
      "args": [],
      "env": {
        "MCP_MEMORY_PATH": "/ABSOLUTE/PATH/TO/YOUR/DATA/DIR"
      }
    }
  }
}
```

### Option B: Local Dev Setup

**1. Clone and Setup**

```bash
git clone https://github.com/iamjpsharma/fremem.git
cd fremem

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies AND the package in editable mode
pip install -e .
```

**2. Configure Windsurf / VS Code (Local Dev)**

Add this to your `mcpServers` configuration (e.g., `~/.codeium/windsurf/mcp_config.json`):

**Note:** Replace `/ABSOLUTE/PATH/TO/fremem` with the actual full path to the cloned directory.

```json
{
  "mcpServers": {
    "memory": {
      "command": "/ABSOLUTE/PATH/TO/fremem/.venv/bin/python",
      "args": ["-m", "fremem.server"],
      "env": {
        "MCP_MEMORY_PATH": "/ABSOLUTE/PATH/TO/fremem/mcp_memory_data"
      }
    }
  }
}
```

## üöÄ Usage

### 0. HTTP Server (New)

You can run the server via HTTP (SSE) if you prefer:

```bash
# Run on port 8000
python -m fremem.server_http
```

Access the SSE endpoint at `http://localhost:8000/sse` and send messages to `http://localhost:8000/messages`.

### üê≥ Run with Docker

To run the server in a container:

```bash
# Build the image
docker build -t fremem .

# Run the container
# Mount your local data directory to /data inside the container
docker run -p 8000:8000 -v $(pwd)/mcp_memory_data:/data fremem
```

The server will be available at `http://localhost:8000/sse`.

### 1. Ingestion (Adding Context)

Use the included helper script `ingest.sh` to add files to a specific project.

```bash
# ingest.sh <project_name> <file1> <file2> ...

# Example: Project "Thaama"
./ingest.sh project-thaama \
  docs/architecture.md \
  src/main.py

# Example: Project "OpenClaw"
./ingest.sh project-openclaw \
  README.md \
  CONTRIBUTING.md
```

### üí° Project ID Naming Convention

It is recommended to use a consistent prefix for your project IDs to avoid collisions:

- `project-thaama`
- `project-openclaw`
- `project-myapp`

### 2. Connect in Editor

Once configured, the following tools will be available to the AI Assistant:

- **`memory_search(project_id, q, filter=None)`**: Semantic search. Supports metadata filtering (e.g., `filter={"type": "code"}`). Returns distance scores.
- **`memory_add(project_id, id, text)`**: Manual addition.
- **`memory_list_sources(project_id)`**: specific files ingested.
- **`memory_delete_source(project_id, source)`**: Remove a specific file.
- **`memory_stats(project_id)`**: Get chunk count.
- **`memory_reset(project_id)`**: Clear all memories for a project.

The AI will effectively have "long-term memory" of the files you ingested.

## üõ† Troubleshooting

- **"No MCP server found" or Connection errors**:
  - Check the output of `pwd` to ensure your absolute paths in `mcp_config.json` are 100% correct.
  - Ensure the virtual environment (`.venv`) is created and dependencies are installed.

- **"Wrong project_id used"**:
  - The AI sometimes guesses the project ID. You can explicitly tell it: "Use project_id 'project-thaama'".

- **Embedding Model Downloads**:
  - On the first run, the server downloads the `all-MiniLM-L6-v2` model (approx 100MB). This may cause a slight delay on the first request.

## üìÅ Repo Structure

```
/
‚îú‚îÄ‚îÄ src/fremem/
‚îÇ   ‚îú‚îÄ‚îÄ server.py       # Main MCP server entry point
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py       # Ingestion logic
‚îÇ   ‚îî‚îÄ‚îÄ db.py           # LanceDB wrapper
‚îú‚îÄ‚îÄ ingest.sh           # Helper script
‚îú‚îÄ‚îÄ requirements.txt    # Top-level dependencies
‚îú‚îÄ‚îÄ pyproject.toml      # Package config
‚îú‚îÄ‚îÄ mcp_memory_data/    # Persistent vector storage (gitignored)
‚îî‚îÄ‚îÄ README.md
```

## üó∫Ô∏è Roadmap

### ‚úÖ Completed (v0.1.x)
- [x] Local vector storage (LanceDB)
- [x] Multi-project isolation
- [x] Markdown ingestion
- [x] PDF ingestion
- [x] Semantic chunking strategies
- [x] Windows support + editable install fixes
- [x] HTTP transport wrapper (SSE)
- [x] Fix resource listing errors (clean MCP UX)
- [x] Robust docs + 5-minute setup
- [x] Multi-IDE support (Windsurf, Cursor-compatible MCP)

### üöÄ Near-Term (v0.2.x ‚Äì Production Readiness)
**üß† Memory Governance**
- [x] List memory sources per project
- [x] Delete memory by source (file-level deletion)
- [x] Reset memory per project
- [x] Replace / reindex mode (prevent stale chunks)
- [x] Memory stats (chunk count, last updated, size)

**üéØ Retrieval Quality**
- [x] Metadata filtering (e.g., type=decision | rules | context)
- [x] Similarity scoring in results
- [ ] Hybrid search (semantic + keyword)
- [ ] Return evidence + similarity scores with search results
- [ ] Configurable top_k defaults per project

**‚öôÔ∏è Dev Workflow**
- [ ] Auto-ingest on git commit / file change
- [ ] `mcp-memory init <project-id>` bootstrap command
- [ ] Project templates (PROJECT_CONTEXT.md, DECISIONS.md, AI_RULES.md)

### üß† Advanced RAG (v0.3.x ‚Äì Differentiators)
- [ ] Hierarchical retrieval (summary-first, detail fallback)
- [ ] Memory compression (old chunks ‚Üí summaries)
- [ ] Temporal ranking (prefer newer decisions)
- [ ] Scoped retrieval (planner vs coder vs reviewer agents)
- [ ] Query rewrite / expansion for better recall

### üè¢ Team / SaaS Mode (Optional)
*Philosophy: Local-first remains the default. SaaS is an optional deployment mode.*

**üîê Auth & Multi-Tenancy**
- [ ] Project-level auth (API keys or JWT)
- [ ] Org / team separation
- [ ] Audit logs for memory changes

**‚òÅÔ∏è Remote Storage Backends (Pluggable)**
- [ ] S3-compatible vector store backend
- [ ] Postgres / pgvector backend
- [ ] Sync & Federation (Local ‚Üî Remote)

### üö´ Non-Goals
- ‚ùå No mandatory cloud dependency
- ‚ùå No vendor lock-in
- ‚ùå No chat history as ‚Äúmemory‚Äù by default (signal > noise)
- ‚ùå No model fine-tuning
